{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "5e50cd42-39f4-44b5-8926-40eba1f90f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "0c236a90-2028-47f4-8450-d2b427032267",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/interim/all_train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "c9746b4e-d7b1-4b1d-a9a8-47fee0ecfc5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id',\n",
       "       'count_floors_pre_eq', 'age', 'area_percentage', 'height_percentage',\n",
       "       'land_surface_condition', 'foundation_type', 'roof_type',\n",
       "       'ground_floor_type', 'other_floor_type', 'position',\n",
       "       'plan_configuration', 'has_superstructure_adobe_mud',\n",
       "       'has_superstructure_mud_mortar_stone', 'has_superstructure_stone_flag',\n",
       "       'has_superstructure_cement_mortar_stone',\n",
       "       'has_superstructure_mud_mortar_brick',\n",
       "       'has_superstructure_cement_mortar_brick', 'has_superstructure_timber',\n",
       "       'has_superstructure_bamboo', 'has_superstructure_rc_non_engineered',\n",
       "       'has_superstructure_rc_engineered', 'has_superstructure_other',\n",
       "       'legal_ownership_status', 'count_families', 'has_secondary_use',\n",
       "       'has_secondary_use_agriculture', 'has_secondary_use_hotel',\n",
       "       'has_secondary_use_rental', 'has_secondary_use_institution',\n",
       "       'has_secondary_use_school', 'has_secondary_use_industry',\n",
       "       'has_secondary_use_health_post', 'has_secondary_use_gov_office',\n",
       "       'has_secondary_use_use_police', 'has_secondary_use_other',\n",
       "       'damage_grade'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "93b6e2dc-0f06-4402-ac5c-39f8e34bf7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0\n",
      "(260601, 39)\n",
      "(247044, 39)\n"
     ]
    }
   ],
   "source": [
    "pct = np.percentile(data.loc[:, 'area_percentage'].fillna(np.mean(data.loc[:, 'area_percentage'])), 95)\n",
    "print(pct)\n",
    "print(data.shape)\n",
    "data = data.loc[data.loc[:, 'area_percentage'] < pct]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "dce0615b-14b6-4abb-9693-594170945b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import numpy as np\n",
    "#height_percentage\n",
    "# Function to create interaction terms\n",
    "def create_interaction_terms(X):\n",
    "    # Assuming X is a dataframe with the columns geo_level_1_id, geo_level_2_id, geo_level_3_id\n",
    "    X_new = X.copy()\n",
    "    #X_new['areaHeight'] = X_new['height_percentage'] / X_new['area_percentage']\n",
    "    X_new['geo1times2'] = X_new['geo_level_1_id'] * X_new['geo_level_2_id']\n",
    "    X_new['geo1times3'] = X_new['geo_level_1_id'] * X_new['geo_level_3_id']\n",
    "    X_new['geo2times3'] = X_new['geo_level_2_id'] * X_new['geo_level_3_id']\n",
    "    X_new['geoall'] = X_new['geo_level_2_id'] * X_new['geo_level_3_id'] * X_new['geo_level_1_id']\n",
    "    return X_new[['geo1times2', 'geo1times3', 'geo2times3', 'geoall']]#, 'areaHeight']]  # Return the interaction terms only\n",
    "\n",
    "# Creating the FunctionTransformer\n",
    "interaction_transformer = FunctionTransformer(create_interaction_terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "e40ef06e-9560-4fcf-84ab-4769f72f9cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geo_level_3_id\n",
      "6536     1\n",
      "6776     1\n",
      "5812     1\n",
      "10748    1\n",
      "4396     1\n",
      "        ..\n",
      "8635     1\n",
      "12567    1\n",
      "5102     1\n",
      "4178     1\n",
      "3085     1\n",
      "Name: count, Length: 963, dtype: int64\n",
      "963\n"
     ]
    }
   ],
   "source": [
    "numerical_df = data.select_dtypes(exclude=['object'])\n",
    "numerical_df.describe()\n",
    "counts = numerical_df['geo_level_3_id'].value_counts()\n",
    "\n",
    "min_count = counts.min()\n",
    "least_frequent_numbers = counts[counts == min_count]\n",
    "print(least_frequent_numbers)\n",
    "print(least_frequent_numbers.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "f3567d6b-eff7-4861-ba8f-f1615942fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "def get_right_skewed_columns(df, skew_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Returns the names of columns that are right-skewed based on the skewness value, excluding binary columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The input DataFrame (numerical columns only).\n",
    "    - skew_threshold: The skewness threshold above which a column is considered right-skewed (default is 0.5).\n",
    "    \n",
    "    Returns:\n",
    "    - List of column names that are right-skewed.\n",
    "    \"\"\"\n",
    "    right_skewed_columns = []\n",
    "    \n",
    "    # Iterate through each column in the dataframe\n",
    "    for col in df.columns:\n",
    "        # Check if the column has more than 2 unique values (to avoid binary columns)\n",
    "        if df[col].nunique() > 2:\n",
    "            # Calculate skewness for each column\n",
    "            col_skewness = skew(df[col].dropna())  # Drop NaN values to avoid issues\n",
    "            \n",
    "            # Check if the skewness is above the specified threshold (indicating right-skewness)\n",
    "            if col_skewness > skew_threshold:\n",
    "                right_skewed_columns.append(col)\n",
    "    \n",
    "    return right_skewed_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d0f2e52e-cd8a-4bef-b7ee-8203b14255a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right-skewed columns: ['count_floors_pre_eq', 'age', 'area_percentage', 'height_percentage', 'count_families']\n"
     ]
    }
   ],
   "source": [
    "# # Select numerical columns\n",
    "# numerical_df = data.select_dtypes(exclude=['object'])\n",
    "\n",
    "# # Get the right-skewed columns\n",
    "right_skewed_cols = get_right_skewed_columns(numerical_df)\n",
    "\n",
    "print(\"Right-skewed columns:\", right_skewed_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "0b91e41a-e47d-4d42-a792-9c23b1484604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def apply_log_transformation(df, columns):\n",
    "#     \"\"\"\n",
    "#     Applies log transformation to the specified columns of the DataFrame.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - df: The input DataFrame.\n",
    "#     - columns: List of column names to apply the log transformation on.\n",
    "    \n",
    "#     Returns:\n",
    "#     - DataFrame with log-transformed columns.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     df_transformed = df.copy()\n",
    "    \n",
    "#     # Apply log transformation to each specified column\n",
    "#     for col in columns:\n",
    "#         # Add a small constant to avoid log(0) and handle any zeros or negatives\n",
    "#         df_transformed[col] = np.log1p([col])\n",
    "    \n",
    "#     return df_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "80619d75-f6d4-4a9a-9b85-4ae1bea084b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the right-skewed columns (excluding binary columns)\n",
    "#right_skewed_cols = get_right_skewed_columns(numerical_df)\n",
    "#numerical_df = apply_log_transformation(numerical_df, right_skewed_cols)\n",
    "#data[right_skewed_cols] = numerical_df[right_skewed_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "70b59429-e3a5-4659-a690-451343d1a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pct = np.percentile(data.loc[:, 'age'].fillna(np.mean(data.loc[:, 'age'])), 99)\n",
    "#print(pct)\n",
    "#print(data.shape)\n",
    "#data = data.loc[data.loc[:, 'age'] < pct]\n",
    "#print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "a3911ffc-6f59-459a-919b-a0331862a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform(X):\n",
    "    # Apply log1p transformation (log(1 + x)) to avoid issues with zero values\n",
    "    return np.log1p(X)\n",
    "\n",
    "# Create a FunctionTransformer for log transformation\n",
    "log_transformer = FunctionTransformer(log_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "58e3fc36-e7c3-4402-94f1-e6ff2ca0a334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "# Custom transformer for the age-based transformation\n",
    "class AgeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, age_column='age'):\n",
    "        self.age_column = age_column\n",
    "        self.percentile_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Calculate the 99th percentile of the 'age' column and store it\n",
    "        self.percentile_ = np.percentile(X[self.age_column].fillna(np.mean(X[self.age_column])), 99)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # Add a new 'old' column to indicate if the age exceeds the 99th percentile\n",
    "        X_copy['old'] = np.where(X_copy[self.age_column] >= self.percentile_, 1, 0)\n",
    "        \n",
    "        # Cap the age to 100 where the 'old' column is 1\n",
    "        X_copy.loc[X_copy['old'] == 1, self.age_column] = 100\n",
    "        \n",
    "        return X_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "8ade25df-1961-4cd1-8288-7d019804fd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop(columns=['damage_grade'])\n",
    "y = data.damage_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "b8cd2381-4b97-4e93-b311-2de1edc0cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.replace({1: 0, 2: 1, 3: 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "dba1291f-ff62-40e5-944a-6372b3517406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2\n",
       "1         1\n",
       "2         2\n",
       "3         1\n",
       "4         2\n",
       "         ..\n",
       "260596    1\n",
       "260597    2\n",
       "260598    2\n",
       "260599    1\n",
       "260600    2\n",
       "Name: damage_grade, Length: 247044, dtype: int64"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "f1c9c93a-0124-4e52-8c6f-1f0d62805c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "damage_grade\n",
       "1    141177\n",
       "2     84323\n",
       "0     21544\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "f62d6401-05c2-4de0-86e9-9a012300ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x.old.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "0243ff84-fb14-4ecb-a5cf-953277152a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical_df = data.select_dtypes(exclude=['object'])\n",
    "#categorical_df = data.select_dtypes(include=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "ca1d5990-7836-4ff9-9cdf-689ddc2445c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "categorical_cols = [cname for cname in x.columns if  x[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in x.columns if x[cname].dtype in ['int32', 'int64', 'float64']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "83ebf2c7-490c-43b9-811f-3c2d149b9f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['land_surface_condition',\n",
       " 'foundation_type',\n",
       " 'roof_type',\n",
       " 'ground_floor_type',\n",
       " 'other_floor_type',\n",
       " 'position',\n",
       " 'plan_configuration',\n",
       " 'legal_ownership_status']"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "1b4ea3f2-1ccf-462f-b581-c21426c40fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(numerical_df.age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "712173a0-97d3-4475-b68f-a303af290e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder,StandardScaler, MinMaxScaler\n",
    "\n",
    "#train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42,  stratify=y) #shuffle=True,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "5390eb12-6721-40bc-a3e9-f60d922a8323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "damage_grade\n",
       "1    98823\n",
       "2    59026\n",
       "0    15081\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "aca1c3dc-856a-47b7-8258-2657b4318b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "damage_grade\n",
       "1    42354\n",
       "2    25297\n",
       "0     6463\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "ca8125c8-9a46-4a44-b21d-d48ce2f8478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import BaseNEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "dddedeba-4ffe-4e0e-a8b8-ce15fe7864a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.eda_first import summarize_dataframe\n",
    "#summarize_dataframe(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "ffc2a3a0-b3d2-49f0-90c6-4474abe506e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create numerical transformer\n",
    "\n",
    "numerical_transformer = Pipeline([('imputer', SimpleImputer(strategy='mean'))])\n",
    "\n",
    "#create categorical transformer\n",
    "#categorical_transformer = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#                                            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "#                                            ])\n",
    "\n",
    "base_encoder_columns = ['land_surface_condition','geo_level_1_id', 'geo_level_2_id','geo_level_3_id',\n",
    " 'foundation_type',\n",
    " 'roof_type',\n",
    " 'ground_floor_type',\n",
    " 'other_floor_type',\n",
    " 'position',\n",
    " 'plan_configuration',\n",
    " 'legal_ownership_status']\n",
    "\n",
    "base_encoder = Pipeline(steps=[\n",
    "    ('base_encoder', BaseNEncoder(cols=base_encoder_columns, base=3))\n",
    "])\n",
    "\n",
    "age_transformer = Pipeline(steps=[\n",
    "    ('age_transform', AgeTransformer(age_column='age'))  # Apply age transformation\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "69e4328c-afab-40e1-a982-c79bb93326ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the transformations using ColumnTransformer\n",
    "#preprocessor = ColumnTransformer(transformers=[\n",
    "#    ('base_name', base_encoder, base_encoder_columns)])  # TargetEncoder for 'town'\n",
    "#    ('num', numerical_transformer, numerical_cols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "ee3cdd35-41ef-4fc8-ab5f-3a6a69cf8ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated ColumnTransformer with the log transformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('base_name', base_encoder, base_encoder_columns),  # BaseNEncoder for categorical columns\n",
    "    ('age_transform', age_transformer, ['age']),  # Custom transformer for the 'age' column\n",
    "    ('num', 'passthrough', numerical_cols),  # Pass numerical columns through without transformation\n",
    "    ('interaction', interaction_transformer, ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id', 'height_percentage', 'area_percentage']),  # Interaction terms\n",
    "    ('log_transform', log_transformer, right_skewed_cols)  # Apply log transformation to specified columns\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "34248e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.60      0.71     12048\n",
      "           1       0.79      0.90      0.84     79123\n",
      "           2       0.82      0.69      0.75     47173\n",
      "\n",
      "    accuracy                           0.80    138344\n",
      "   macro avg       0.82      0.73      0.77    138344\n",
      "weighted avg       0.81      0.80      0.80    138344\n",
      "\n",
      "Test Set Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.45      0.55      6463\n",
      "           1       0.74      0.85      0.79     42354\n",
      "           2       0.76      0.63      0.69     25297\n",
      "\n",
      "    accuracy                           0.74     74114\n",
      "   macro avg       0.73      0.64      0.68     74114\n",
      "weighted avg       0.74      0.74      0.74     74114\n",
      "\n",
      "Accuracy: 0.7422349353698356\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Split the data into training and validation sets for cross-validation\n",
    "X_train_full, X_val, y_train_full, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model 1: Binary Classification (0 vs Others)\n",
    "xgb_binary = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.2669112505018992,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    objective='binary:logistic'\n",
    ")\n",
    "\n",
    "# Model 2: Binary Classification (1 vs 2)\n",
    "xgb_multi = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.2669112505018992,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    objective='binary:logistic'\n",
    ")\n",
    "\n",
    "# Assuming preprocessor is defined elsewhere\n",
    "model1_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # Step 1: Preprocessing\n",
    "    ('xgboost', xgb_binary)  # Step 2: Binary Classification\n",
    "])\n",
    "\n",
    "# Hyperparameter tuning for Model 1\n",
    "param_grid = {\n",
    "    'xgboost__n_estimators': [100, 300, 500],\n",
    "    'xgboost__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'xgboost__max_depth': [3, 5, 7]\n",
    "}\n",
    "grid_search = GridSearchCV(model1_pipe, param_grid, cv=5, scoring='f1_micro')\n",
    "grid_search.fit(X_train_full, y_train_full != 0)\n",
    "best_model1_pipe = grid_search.best_estimator_\n",
    "\n",
    "# Predict with Model 1 on train set\n",
    "y_pred_model1_train = best_model1_pipe.predict(X_train_full)\n",
    "\n",
    "# Filter data for Model 2 (predicted as \"others\" by Model 1 on train set)\n",
    "X_other_train = X_train_full[y_pred_model1_train == 1]  # Assuming predicted class 1 means \"others\"\n",
    "y_other_train = y_train_full[y_pred_model1_train == 1]\n",
    "\n",
    "# Map y_other_train to binary labels (1 -> 0, 2 -> 1)\n",
    "y_other_binary_train = (y_other_train == 2).astype(int)\n",
    "\n",
    "# Train Model 2 on filtered train data\n",
    "model2_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # Step 1: Preprocessing\n",
    "    ('xgboost', xgb_multi)  # Step 2: Binary Classification (1 vs 2)\n",
    "])\n",
    "model2_pipe.fit(X_other_train, y_other_binary_train)\n",
    "\n",
    "# Predict with Model 2 on filtered train data\n",
    "y_pred_model2_train = model2_pipe.predict(X_other_train)\n",
    "\n",
    "# Final prediction based on Model 1 & 2 outputs for train set\n",
    "final_preds_train = np.zeros_like(y_train_full)\n",
    "final_preds_train[y_pred_model1_train == 0] = 0  # Class 0 from Model 1\n",
    "final_preds_train[y_pred_model1_train == 1] = y_pred_model2_train + 1  # Class 1 or 2 from Model 2\n",
    "\n",
    "# Evaluate the final predictions on train set\n",
    "train_report = classification_report(y_train_full, final_preds_train)\n",
    "print(\"Train Set Classification Report:\\n\", train_report)\n",
    "\n",
    "# Predict with Model 1 on test set\n",
    "y_pred_model1_test = best_model1_pipe.predict(X_test)\n",
    "\n",
    "# Filter data for Model 2 (predicted as \"others\" by Model 1 on test set)\n",
    "X_other_test = X_test[y_pred_model1_test == 1]  # Assuming predicted class 1 means \"others\"\n",
    "y_other_test = y_test[y_pred_model1_test == 1]\n",
    "\n",
    "# Map y_other_test to binary labels (1 -> 0, 2 -> 1)\n",
    "y_other_binary_test = (y_other_test == 2).astype(int)\n",
    "\n",
    "# Predict with Model 2 on filtered test data\n",
    "y_pred_model2_test = model2_pipe.predict(X_other_test)\n",
    "\n",
    "# Final prediction based on Model 1 & 2 outputs for test set\n",
    "final_preds_test = np.zeros_like(y_test)\n",
    "final_preds_test[y_pred_model1_test == 0] = 0  # Class 0 from Model 1\n",
    "final_preds_test[y_pred_model1_test == 1] = y_pred_model2_test + 1  # Class 1 or 2 from Model 2\n",
    "\n",
    "# Evaluate the final predictions on test set\n",
    "test_report = classification_report(y_test, final_preds_test)\n",
    "print(\"Test Set Classification Report:\\n\", test_report)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, final_preds_test)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "29a0eb0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Pipeline.get_params of Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('base_name',\n",
       "                                                  Pipeline(steps=[('base_encoder',\n",
       "                                                                   BaseNEncoder(base=3,\n",
       "                                                                                cols=['land_surface_condition',\n",
       "                                                                                      'geo_level_1_id',\n",
       "                                                                                      'geo_level_2_id',\n",
       "                                                                                      'geo_level_3_id',\n",
       "                                                                                      'foundation_type',\n",
       "                                                                                      'roof_type',\n",
       "                                                                                      'ground_floor_type',\n",
       "                                                                                      'other_floor_type',\n",
       "                                                                                      'position',\n",
       "                                                                                      'plan_configuration',\n",
       "                                                                                      'legal_ownership_status']))]),\n",
       "                                                  ['land_s...\n",
       "                               feature_types=None, gamma=None, grow_policy=None,\n",
       "                               importance_type=None,\n",
       "                               interaction_constraints=None, learning_rate=0.1,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=None,\n",
       "                               max_depth=7, max_leaves=None,\n",
       "                               min_child_weight=None, missing=nan,\n",
       "                               monotone_constraints=None, multi_strategy=None,\n",
       "                               n_estimators=300, n_jobs=None,\n",
       "                               num_parallel_tree=None, random_state=42, ...))])>"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "04b5f9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.max of 107180    1\n",
       "257217    1\n",
       "191728    0\n",
       "134859    1\n",
       "35271     1\n",
       "         ..\n",
       "249701    1\n",
       "218422    1\n",
       "130372    1\n",
       "2939      1\n",
       "6423      1\n",
       "Name: damage_grade, Length: 74114, dtype: int64>"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d3a572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "f5ad501a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "damage_grade\n",
       "1    141177\n",
       "2     84323\n",
       "0     21544\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "daeb6fd1-1fb3-4479-8978-2a38fc3f9f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/raw/test_values.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "dafbc507-fb2c-4ad2-8134-70d35d158954",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final = test_data.building_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "730a579f-3081-41a2-85ae-7ebc758060d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the new test data\n",
    "test_data = pd.read_csv('../data/raw/test_values.csv')\n",
    "#test_data = X_test\n",
    "# Predict with Model 1 on new test data\n",
    "y_pred_model1_test_new = best_model1_pipe.predict(test_data)\n",
    "\n",
    "# Filter data for Model 2 (predicted as \"others\" by Model 1 on new test data)\n",
    "X_other_test_new = test_data[y_pred_model1_test_new == 1]  # Assuming predicted class 1 means \"others\"\n",
    "\n",
    "# Predict with Model 2 on filtered new test data\n",
    "if len(X_other_test_new) > 0:\n",
    "    y_pred_model2_test_new = model2_pipe.predict(X_other_test_new)\n",
    "else:\n",
    "    y_pred_model2_test_new = np.array([])\n",
    "\n",
    "# Final prediction based on Model 1 & 2 outputs for new test data\n",
    "final_preds_test_new = np.zeros(len(test_data), dtype=int)\n",
    "final_preds_test_new[y_pred_model1_test_new == 0] = 0  # Class 0 from Model 1\n",
    "if len(X_other_test_new) > 0:\n",
    "    final_preds_test_new[y_pred_model1_test_new == 1] = y_pred_model2_test_new + 1  # Class 1 or 2 from Model 2\n",
    "\n",
    "\n",
    "print(final_preds_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "8871625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_preds = pd.Series(final_preds_test_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "80bebbfa-d40f-41b9-9103-a3dfca239303",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concatenated = pd.concat([X_test_final, rf_preds], axis=1)\n",
    "df_concatenated\n",
    "df_concatenated = df_concatenated.rename(columns={0: 'damage_grade'})\n",
    "df_concatenated\n",
    "df_concatenated['damage_grade'] = df_concatenated['damage_grade'].replace({0: 1, 1: 2, 2: 3})\n",
    "df_concatenated.to_csv('../data/processed/ATsubmission2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28226e9-3c26-44cb-beda-9c50233aadc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
