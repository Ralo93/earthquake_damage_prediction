import pandas as pd
import os
import numpy as np
import sys
sys.path.append('..')


data = pd.read_csv('../data/interim/all_train_data.csv')


#data.height_percentage.value_counts()


data.columns


pct = np.percentile(data.loc[:, 'area_percentage'].fillna(np.mean(data.loc[:, 'area_percentage'])), 97)
print(pct)
print(data.shape)
data = data.loc[data.loc[:, 'area_percentage'] < pct]
print(data.shape)


pct = np.percentile(data.loc[:, 'height_percentage'].fillna(np.mean(data.loc[:, 'height_percentage'])), 97)
print(pct)
print(data.shape)
data = data.loc[data.loc[:, 'height_percentage'] < pct]
print(data.shape)





from sklearn.base import BaseEstimator, TransformerMixin
import numpy as np

class GeoInteractionTransformer(BaseEstimator, TransformerMixin):
    """
    Custom transformer to create geo interaction terms by concatenating the geo-level IDs.
    """
    def fit(self, X, y=None):
        return self
    
    def transform(self, X, y=None):
        X_new = X.copy()
        # Concatenate geo_level_1_id, geo_level_2_id, and geo_level_3_id
        X_new['geo1_geo2'] = X_new['geo_level_1_id'].astype(str) + '_' + X_new['geo_level_2_id'].astype(str)
        X_new['geo1_geo3'] = X_new['geo_level_1_id'].astype(str) + '_' + X_new['geo_level_3_id'].astype(str)
        X_new['geo2_geo3'] = X_new['geo_level_2_id'].astype(str) + '_' + X_new['geo_level_3_id'].astype(str)
        X_new['geo_all'] = (
            X_new['geo_level_1_id'].astype(str) + '_' + 
            X_new['geo_level_2_id'].astype(str) + '_' +
            X_new['geo_level_3_id'].astype(str)
        )
        # Return the entire dataframe including original and new columns
        return X_new


numerical_df = data.select_dtypes(exclude=['object'])
numerical_df.describe()


from scipy.stats import skew

def get_right_skewed_columns(df, skew_threshold=0.5):
    """
    Returns the names of columns that are right-skewed based on the skewness value, excluding binary columns.
    
    Parameters:
    - df: The input DataFrame (numerical columns only).
    - skew_threshold: The skewness threshold above which a column is considered right-skewed (default is 0.5).
    
    Returns:
    - List of column names that are right-skewed.
    """
    right_skewed_columns = []
    
    # Iterate through each column in the dataframe
    for col in df.columns:
        # Check if the column has more than 2 unique values (to avoid binary columns)
        if df[col].nunique() > 2:
            # Calculate skewness for each column
            col_skewness = skew(df[col].dropna())  # Drop NaN values to avoid issues
            
            # Check if the skewness is above the specified threshold (indicating right-skewness)
            if col_skewness > skew_threshold:
                right_skewed_columns.append(col)
    
    return right_skewed_columns



# # Select numerical columns
# numerical_df = data.select_dtypes(exclude=['object'])

# # Get the right-skewed columns
right_skewed_cols = get_right_skewed_columns(numerical_df)

print("Right-skewed columns:", right_skewed_cols)



from sklearn.preprocessing import FunctionTransformer

def log_transform(X):
    # Apply log1p transformation (log(1 + x)) to avoid issues with zero values
    return np.log1p(X)

# Create a FunctionTransformer for log transformation
log_transformer = FunctionTransformer(log_transform)



from sklearn.base import BaseEstimator, TransformerMixin
import numpy as np

# Custom transformer for the age-based transformation
class AgeTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, age_column='age'):
        self.age_column = age_column
        self.percentile_ = None

    def fit(self, X, y=None):
        # Calculate the 99th percentile of the 'age' column and store it
        self.percentile_ = np.percentile(X[self.age_column].fillna(np.mean(X[self.age_column])), 99)
        return self

    def transform(self, X):
        X_copy = X.copy()
        
        # Add a new 'old' column to indicate if the age exceeds the 99th percentile
        X_copy['old'] = np.where(X_copy[self.age_column] >= self.percentile_, 1, 0)
        
        # Cap the age to 100 where the 'old' column is 1
        X_copy.loc[X_copy['old'] == 1, self.age_column] = 100
        
        return X_copy



x = data.drop(columns=['damage_grade'])
y = data.damage_grade


y = y.replace({1: 0, 2: 1, 3: 2})


y


y.value_counts()


#x.old.value_counts()


#numerical_df = data.select_dtypes(exclude=['object'])
#categorical_df = data.select_dtypes(include=['object'])


# Select categorical columns with relatively low cardinality (convenient but arbitrary)
categorical_cols = [cname for cname in x.columns if  x[cname].dtype == "object"]

# Select numerical columns
numerical_cols = [cname for cname in x.columns if x[cname].dtype in ['int32', 'int64', 'float64']]


categorical_cols


#plt.hist(numerical_df.age)


from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder,StandardScaler, MinMaxScaler

#train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42, shuffle=True)


y_train.value_counts()


y_test.value_counts()


from category_encoders import BaseNEncoder


from sklearn.base import BaseEstimator, TransformerMixin

class SuperstructureFeatureTransformer(BaseEstimator, TransformerMixin):
    """
    Custom transformer to split superstructure-related columns into new features 
    like 'has_stone', 'has_brick', 'has_mortar', 'has_cement', and 'has_mud'.
    """
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X_copy = X.copy()
        
        # Create the new "has_stone" feature
        X_copy['has_stone'] = (
            X_copy['has_superstructure_mud_mortar_stone'] | 
            X_copy['has_superstructure_stone_flag'] | 
            X_copy['has_superstructure_cement_mortar_stone']
        )
        
        # Create the new "has_brick" feature
        X_copy['has_brick'] = (
            X_copy['has_superstructure_mud_mortar_brick'] | 
            X_copy['has_superstructure_cement_mortar_brick']
        )
        
        # Create the new "has_adobe_mud" feature
        #X_copy['has_adobe_mud'] = X_copy['has_superstructure_adobe_mud']
        
        # Create the new "has_mortar" feature
        X_copy['has_mortar'] = (
            X_copy['has_superstructure_mud_mortar_stone'] | 
            X_copy['has_superstructure_mud_mortar_brick']
        )
        
        # Create the new "has_cement" feature
        X_copy['has_cement'] = (
            X_copy['has_superstructure_cement_mortar_stone'] | 
            X_copy['has_superstructure_cement_mortar_brick']
        )
        
        # Create the new "has_mud" feature
        X_copy['has_mud'] = (
            X_copy['has_superstructure_mud_mortar_stone'] | 
            X_copy['has_superstructure_mud_mortar_brick'] | 
            X_copy['has_superstructure_adobe_mud']
        )
        
        return X_copy



# #create numerical transformer
geo_interaction_transformer = GeoInteractionTransformer()

geo_cols = ['geo1_geo2', 'geo1_geo3', 'geo2_geo3', 'geo_all']

# # # BaseNEncoder for the high cardinality of interaction features
base_encoder_geo = BaseNEncoder(cols=geo_cols, base=5)

# # Geo interaction pipeline: First, create interaction terms, then encode them
geo_interaction_pipeline = Pipeline(steps=[
     ('geo_interaction', geo_interaction_transformer),  # Create interaction terms
     ('base_encoder', base_encoder_geo)  # Encode interaction terms
])


# numerical_transformer = Pipeline([('imputer', SimpleImputer(strategy='mean'))])

# #create categorical transformer
# #categorical_transformer = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='most_frequent')),
# #                                            ('onehot', OneHotEncoder(handle_unknown='ignore'))
# #                                            ])

#reduction_columns = ['geo_level_1_id', 'geo_level_2_id','geo_level_3_id']


# base_encoder = Pipeline(steps=[
#     ('base_encoder', BaseNEncoder(cols=reduction_columns, base=3))
# ])

# age_transformer = Pipeline(steps=[
#     ('age_transform', AgeTransformer(age_column='age'))  # Apply age transformation
# ])





data.columns





import torch
import torch.nn as nn
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer
import pandas as pd

# Define the autoencoder class
class Autoencoder(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super(Autoencoder, self).__init__()
        # Encoder
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, latent_dim)
        )
        # Decoder (optional, only necessary if you want reconstruction)
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.ReLU(),
            nn.Linear(128, input_dim)
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return encoded, decoded

# Custom transformer to handle autoencoder training and dimensionality reduction
class AutoencoderTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, latent_dim=10, epochs=100, batch_size=32, learning_rate=1e-3):
        self.latent_dim = latent_dim
        self.epochs = epochs
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.autoencoder = None  # Autoencoder model will be initialized in `fit`

    def fit(self, X, y=None):
        # Automatically determine input_dim based on X's shape after BaseN encoding
        input_dim = X.shape[1]  # Number of features in X after BaseN encoding
        self.autoencoder = Autoencoder(input_dim=input_dim, latent_dim=self.latent_dim)

        # Convert X to NumPy array if it's a DataFrame
        if isinstance(X, pd.DataFrame):
            X = X.values

        X_tensor = torch.tensor(X, dtype=torch.float32)  # Convert NumPy array to tensor
        optimizer = torch.optim.Adam(self.autoencoder.parameters(), lr=self.learning_rate)
        criterion = nn.MSELoss()

        # Training loop
        for epoch in range(self.epochs):
            optimizer.zero_grad()
            encoded, decoded = self.autoencoder(X_tensor)
            loss = criterion(decoded, X_tensor)
            loss.backward()
            optimizer.step()

            if epoch % 10 == 0:  # Print loss every 10 epochs for monitoring
                print(f'Epoch {epoch}, Loss: {loss.item()}')
                
        return self

    def transform(self, X):
        # Convert X to NumPy array if it's a DataFrame
        if isinstance(X, pd.DataFrame):
            X = X.values

        self.autoencoder.eval()  # Set to evaluation mode
        X_tensor = torch.tensor(X, dtype=torch.float32)  # Convert NumPy array to tensor
        with torch.no_grad():
            encoded, _ = self.autoencoder(X_tensor)
        return encoded.numpy()  # Return the reduced-dimensional representation


reduction_columns = ['geo_level_1_id', 'geo_level_2_id','geo_level_3_id']

base_encoder_columns = ['land_surface_condition',
 'foundation_type',
 'roof_type',
 'ground_floor_type',
 'other_floor_type',
 'position',
 'plan_configuration',
 'legal_ownership_status']

base_encoder = Pipeline(steps=[
    ('base_encoder', BaseNEncoder(cols=base_encoder_columns, base=3))
])

base_encoder_geo = Pipeline(steps=[
    ('base_encoder', BaseNEncoder(cols=reduction_columns, base=3))
])

# Define the pipeline with an Autoencoder for dimensionality reduction
preprocessor = ColumnTransformer(transformers=[

    ('base_name_geo', base_encoder_geo, reduction_columns),  # BaseN encoding on categorical columns

    # Autoencoder dimensionality reduction for BaseN encoded features
    ('autoencoder', Pipeline(steps=[
        ('extractor', FunctionTransformer(lambda x: x, feature_names_out='one-to-one')),  # Pass through BaseN encoded features
        ('autoencoder', AutoencoderTransformer(latent_dim=5))  # Autoencoder with latent dimension 10
    ]), reduction_columns),
    # ('autoencoder', Pipeline(steps=[
    #     ('extractor', FunctionTransformer(lambda x: x, feature_names_out='one-to-one')),  # Extract original geo columns
    #     ('autoencoder', AutoencoderTransformer(latent_dim=5))  # Autoencoder for geo features with latent dimension 5
    # ]), reduction_columns),

    ('base_name', base_encoder, base_encoder_columns),  # BaseN encoding on categorical columns
    ('age_transform', age_transformer, ['age']),  # Custom transformer for 'age'
    ('num', 'passthrough', numerical_cols),  # Pass through numerical columns without transformation
    ('log_transform', log_transformer, right_skewed_cols)  # Log transform for right-skewed columns
    # Add other transformers or steps as needed
])





# # Define the pipeline with an Autoencoder replacing PCA
# preprocessor = ColumnTransformer(transformers=[
#     ('base_name', base_encoder, base_encoder_columns),  # BaseN encoding on categorical columns

#     # Autoencoder dimensionality reduction for BaseN encoded features
#     ('autoencoder', Pipeline(steps=[
#         ('extractor', FunctionTransformer(lambda x: x, feature_names_out='one-to-one')),  # Extract BaseN encoded features
#         ('autoencoder', AutoencoderTransformer(input_dim=15, latent_dim=10))  # Adjust input_dim to match BaseN output
#     ]), base_encoder_columns),

#     ('age_transform', age_transformer, ['age']),  # Custom transformer for 'age'
#     ('num', 'passthrough', numerical_cols),  # Pass through numerical columns without transformation
#     ('log_transform', log_transformer, right_skewed_cols)  # Log transform for right-skewed columns
# ])



from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score, classification_report
from xgboost import XGBClassifier

# LightGBM for Multiclass Classification
lgbm = LGBMClassifier(
    n_estimators=843,
    learning_rate=0.16934236113095621,
    max_depth=7,
    random_state=42,
    lambda_l1=7.455101924325866,  # L2 regularization
    lambda_l2=0.007915220427757011,
    subsample=0.7354468519750788,    # Subsample ratio of the training instances
    colsample_bytree=0.8648222301238216,           # Subsample ratio of columns when constructing each tree
    min_child_weight=2,             # Equivalent of min_data_in_leaf in LightGBM
    subsample_for_bin=200000,
    num_leaves=31,
    #reg_alpha=0.14170716330946964,  # L1 regularization term
    objective='multiclass',         # Objective for multiclass classification
    metric='multi_logloss',         # Metric used for multiclass classification
    num_class=3                     # Specify the number of classes in the target
)

rf_pipe = Pipeline(steps=[
    ('preprocessor', preprocessor),  # Step 1: Preprocessing
    ('xgboost', lgbm)  # Step 3: Model training
])

# Preprocessing of training data, fit model 
#rf_pipe.fit(X_train, y_train)

# Preprocessing of training data, fit model after upsampling!
rf_pipe.fit(X_train, y_train)

# Preprocessing of validation data, get predictions
rf_preds = rf_pipe.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, rf_preds)
print('Accuracy for XGBoost:', accuracy)

# Detailed classification report
print('Classification Report:\n', classification_report(y_test, rf_preds))


# from sklearn.model_selection import GridSearchCV

# param_grid = {
#     'preprocessor__autoencoder__autoencoder__latent_dim': [5, 10, 15, 20]  # Testing different latent_dim values
# }

# # Create a grid search
# grid_search = GridSearchCV(rf_pipe, param_grid, cv=3)
# grid_search.fit(X_train, y_train)

# # Best latent_dim
# best_latent_dim = grid_search.best_params_['preprocessor__autoencoder__autoencoder__latent_dim']
# print(f'Best latent_dim: {best_latent_dim}')


# from sklearn.metrics import accuracy_score, classification_report
# from xgboost import XGBClassifier
# # XGBoost
# xgb = XGBClassifier(
#     n_estimators=600,
#     learning_rate=0.2669112505018992,
#     max_depth=5,
#     random_state=42,
#     reg_lambda=1.2259716591605452,
#     subsample=0.704976942819638,
#     colsample_bytree=0.9,
#     min_child_weight=4,
#     alpha= 0.14170716330946964,    # Added L1 regularization
#     eval_metric='mlogloss',  # Consider custom loss for ordinal
#     objective='multi:softmax',  # Using softmax but can tweak for ordinal
#     num_class=3  # Assuming 3 ordinal classes
# )

# rf_pipe = Pipeline(steps=[
#     ('preprocessor', preprocessor),  # Step 1: Preprocessing
#     ('xgboost', xgb)  # Step 3: Model training
# ])

# # Preprocessing of training data, fit model 
# #rf_pipe.fit(X_train, y_train)

# # Preprocessing of training data, fit model after upsampling!
# rf_pipe.fit(X_train, y_train)

# # Preprocessing of validation data, get predictions
# rf_preds = rf_pipe.predict(X_test)

# # Evaluate the model
# accuracy = accuracy_score(y_test, rf_preds)
# print('Accuracy for XGBoost:', accuracy)

# # Detailed classification report
# print('Classification Report:\n', classification_report(y_test, rf_preds))


# Preprocessing of validation data, get predictions
rf_preds = rf_pipe.predict(X_train)

# Evaluate the model
accuracy = accuracy_score(y_train, rf_preds)
print('Accuracy for XGBoost:', accuracy)

# Detailed classification report
print('Classification Report:\n', classification_report(y_train, rf_preds))


test_data = pd.read_csv('../data/raw/test_values.csv')


X_test_final = test_data.building_id


X_test_final


rf_preds = rf_pipe.predict(test_data)
rf_preds = pd.Series(rf_preds)
rf_preds 


df_concatenated = pd.concat([X_test_final, rf_preds], axis=1)
df_concatenated
df_concatenated = df_concatenated.rename(columns={0: 'damage_grade'})
df_concatenated
df_concatenated['damage_grade'] = df_concatenated['damage_grade'].replace({0: 1, 1: 2, 2: 3})
df_concatenated.to_csv('../data/processed/RLsub1.csv', index=False)



